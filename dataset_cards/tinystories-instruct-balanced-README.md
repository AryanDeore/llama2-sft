# TinyStories Instruct Balanced

Curated instruction-tuning dataset derived from [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) with balanced happy/sad endings. Designed for fine-tuning small language models (7M-125M parameters) on instruction-following tasks.

## Dataset Details

- **Train Examples**: 324,984
- **Validation Examples**: 3,542
- **Ending Distribution**: 50% Happy / 50% Sad
- **Language**: English
- **Total Tokens**: ~60M (avg 180 tokens per example)
- **License**: CC-BY-4.0

## Data Format

Each example contains:
- `instruction`: Story prompt (e.g., "Write a story about a brave knight")
- `response`: Generated story with happy or sad ending (30-80 tokens)
- `ending`: Label (`happy` or `sad`)

### Example

```json
{
  "instruction": "Write a story about a little boy who found a magical treasure",
  "response": "Once upon a time, there was a little boy named Tom. One day, while exploring the forest, he found a golden chest. Inside was a magical lamp. Tom made a wish and suddenly he was surrounded by his family. They all hugged him and he lived happily ever after.",
  "ending": "happy"
}
```

## Recommended Training Configuration

```python
batch_size = 64
learning_rate = 5e-5
epochs = 3-5
max_length = 256
optimizer = "AdamW"
weight_decay = 0.1
```

**Results (Llama 2 15M):**
- Best epoch: 3
- Final val_loss: 1.1275
- Train time: ~6 hours (1x A100 GPU)

## Usage

Load with ðŸ¤— Hugging Face `datasets`:

```python
from datasets import load_dataset

dataset = load_dataset("0rn0/tinystories-instruct-balanced")

train = dataset["train"]
val = dataset["validation"]
```

## Related Models

Fine-tuned models using this dataset:
- [0rn0/llama2-15m-tinystories-sft](https://huggingface.co/0rn0/llama2-15m-tinystories-sft) â€” Llama 2 15M
- [0rn0/gpt2-30m-tinystories-sft](https://huggingface.co/0rn0/gpt2-30m-tinystories-sft) â€” GPT-2 30M
- [0rn0/gpt2-125m-tinystories-sft](https://huggingface.co/0rn0/gpt2-125m-tinystories-sft) â€” GPT-2 125M

## Limitations

- Small dataset (~325K examples) â€” may not generalize beyond story generation
- Synthetic data â€” all stories generated by instruction-tuned models
- Binary endings â€” only happy/sad classifications
- Limited vocabulary and narrative complexity

## Source

Based on [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) (Eldan & Li, 2023). Restructured into instruction-response format and balanced for controllable story generation.

## Citation

```bibtex
@article{eldan2023tinystories,
  title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author={Eldan, Ronen and Li, Yonatan},
  journal={arXiv preprint arXiv:2305.07759},
  year={2023}
}
```

**Dataset:** TinyStories Instruct Balanced ([0rn0/tinystories-instruct-balanced](https://huggingface.co/datasets/0rn0/tinystories-instruct-balanced))
